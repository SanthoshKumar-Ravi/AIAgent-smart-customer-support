# ğŸ¤– Smart Customer Support AI
A full-stack AI-powered customer support application that answers user queries using a fine-tuned LLM, stores conversations and feedback in Redis, and offers an intuitive Streamlit UI for interaction and feedback collection.

## âœ¨ Features
âœ… LLM-powered real-time Q&A engine using Ollama (LLaMA3)

âœ… Redis for persistent storage of Q&A and feedback data

âœ… LangChain & FAISS-based semantic search

âœ… Streamlit frontend for user interaction

âœ… FastAPI backend to serve AI and feedback endpoints

âœ… Feedback loop for continuous improvement

âœ… Dockerized microservice architecture

## ğŸ› ï¸ Tech Stack

| **Layer**	| **Technology** |
| :---:          |   :---:          |
| Frontend | 	Streamlit |
| Backend	| FastAPI |
| LLM Engine |	Ollama (LLaMA3) |
| Vector Store |	FAISS + LangChain |
|Database |	Redis |
| DevOps |	Docker, Docker Compose |

## ğŸ“ Project Structure
```
llm-support-ai/
â”‚
â”œâ”€â”€ backend/              # FastAPI backend service
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ feedback.py
â”‚   â””â”€â”€ dockerfile
â”‚
â”œâ”€â”€ frontend/             # Streamlit frontend service
â”‚   â””â”€â”€ streamlit_app.py
|   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ requirements.txt      # Common dependencies
â”œâ”€â”€ docker-compose.yml    # Multi-service container orchestration
â””â”€â”€ README.md             # Project documentation
```

## ğŸ§‘â€ğŸ’» Running Locally (Dev Setup)

### âœ… Prerequisites

1. Python 3.11+
2. Redis running on localhost:6379
3. Ollama with LLaMA3 model installed (ollama pull llama3)


### ğŸš€ Step-by-step

**1. Clone the repository**

```
git clone https://github.com/your-username/llm-support-ai.git
```

```
cd llm-support-ai
```

**2. Install dependencies**

```
pip install -r requirements.txt
```

**3. Start Backend**

```
cd backend
uvicorn main:app --reload
```

**4. Start Frontend**

```
cd frontend
streamlit run app.py
```

**Test the App**

Open http://localhost:8501 in your browser and start interacting!

## ğŸ³ Running with Docker (Prod Setup)
### âœ… Prerequisites

1. Docker
2. Docker Compose
3. Ollama running locally (or modify to use Ollama container)

### ğŸš€ Start the Application

```
docker-compose up --build
```
This will:
Start the FastAPI backend on http://localhost:8000
Start the Streamlit frontend on http://localhost:8501

### ğŸ§  Ensure Ollama is Running
If youâ€™re running Ollama locally:

```
ollama run llama3
```
Or include Ollama in the docker-compose.yml if you want a complete containerized setup.

